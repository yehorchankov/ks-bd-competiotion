{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Make label encode and then one-hot encode on data\n",
    "def label_oh_encode(data, feature_names='all'):\n",
    "    data_le = LabelEncoder()\n",
    "    data_encoded = data\n",
    "    if feature_names == 'all':\n",
    "        for feat in list(data):\n",
    "            data_encoded[feat] = data_le.fit_transform(data_encoded[feat])\n",
    "    elif type(feature_names) == list:\n",
    "        for feat in feature_names:\n",
    "            data_encoded[feat] = data_le.fit_transform(data_encoded[feat])\n",
    "        feature_names = get_feature_indices(data_encoded, feature_names)\n",
    "\n",
    "    oh_encoder = OneHotEncoder(categorical_features=feature_names)\n",
    "    data_encoded = oh_encoder.fit_transform(data_encoded).toarray()\n",
    "    df = DataFrame(data=data_encoded)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Get column indices of features for the dataset\n",
    "def get_feature_indices(data, feature_names):\n",
    "    header_index = []\n",
    "    header = list(data)\n",
    "    for name in feature_names:\n",
    "        header_index.append(header.index(name))\n",
    "    return header_index\n",
    "\n",
    "\n",
    "# Add header row for data object of DataFrame type\n",
    "def add_header(data, base_name, additional):\n",
    "    column_names = [(base_name + str(i))\n",
    "                    for i in range(data.shape[1] - len(additional))]\n",
    "    for lbl in additional:\n",
    "        column_names.append(lbl)\n",
    "    data.columns = column_names\n",
    "    return data\n",
    "\n",
    "\n",
    "# Replacing missing values in the data\n",
    "def imput_column_data(data, columns='all', imput_type='median'):\n",
    "    data_imputer = Imputer(strategy=imput_type)\n",
    "    imputed_data = data\n",
    "    if columns == 'all':\n",
    "        imputed_data = data_imputer.fit_transform(imputed_data)\n",
    "    elif type(columns) == list:\n",
    "        for col in columns:\n",
    "            imputed_data[col] = data_imputer.fit_transform(imputed_data[col])\n",
    "\n",
    "    df = DataFrame(data=imputed_data)\n",
    "    df.columns = list(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Select predictors that are best fit \n",
    "def select_predictors(data, target, base_name, oh_length=0, threshold=5.0, k_best=3, col_all=True):\n",
    "    if not col_all:\n",
    "        # Get all column headers, except for ID, Month and TARGET columns\n",
    "        predictors = list(data)[2:-1]\n",
    "    else:\n",
    "        predictors = list(data)\n",
    "\n",
    "    # Select only features with high variance with kbest\n",
    "    selector = SelectKBest(f_classif, k=k_best)\n",
    "    selector.fit(data[predictors], data[target])\n",
    "\n",
    "    # Scale scores logarithmically\n",
    "    scores = -np.log10(selector.pvalues_)\n",
    "    pd_df = DataFrame(data=predictors)\n",
    "    predictors = pd_df.loc[scores >= threshold]\n",
    "    predictors = predictors[0].values.tolist()\n",
    "    predictors = predictors + [(base_name + str(i))\n",
    "                               for i in range(oh_length)]\n",
    "    return predictors\n",
    "\n",
    "\n",
    "# Creates random forest classifier for specified data\n",
    "def create_rnd_forest(n_estim=100):\n",
    "    forest = RandomForestClassifier(\n",
    "        n_estimators=n_estim)\n",
    "    return forest\n",
    "\n",
    "\n",
    "# Creates support vector machines classifier for specified data\n",
    "def create_svc(c=1.0, kernel='rbf'):\n",
    "    svc = SVC(C=c, kernel=kernel)\n",
    "    return svc\n",
    "\n",
    "\n",
    "# Creates bagging classifier with decision tree as estimator for specified data\n",
    "def create_bag_cls(n_estim=200):\n",
    "    dcg_tree = DecisionTreeClassifier(criterion='gini')\n",
    "    bag = BaggingClassifier(\n",
    "        base_estimator=dcg_tree,\n",
    "        n_estimators=n_estim,\n",
    "        n_jobs=-1)\n",
    "    return bag\n",
    "\n",
    "\n",
    "def main():\n",
    "    target_column = 'TARGET'\n",
    "    feature_threshold = 9.0\n",
    "\n",
    "    test_class = pd.read_csv('test.txt', header=0, sep='\\t')\n",
    "    train_class = pd.read_csv('train.txt', header=0, sep='\\t')\n",
    "    base1_feat = pd.read_csv('Base1.txt', header=0, sep='\\t')\n",
    "    base2_feat = pd.read_csv('Base2.txt', header=0, sep='\\t')\n",
    "    base2_feat_oh = label_oh_encode(base2_feat, feature_names=['T1', 'T2', 'T3', 'T4'])\n",
    "    base2_feat_oh = add_header(base2_feat_oh, 'T', ['ID'])\n",
    "    base1_feat_imp = imput_column_data(base1_feat)\n",
    "    base1_feat_imp['key'] = base1_feat_imp['ID']\n",
    "    full_data = base1_feat_imp.groupby('key').mean()\n",
    "\n",
    "    full_data = pd.merge(full_data, base2_feat_oh, on='ID', how='right')\n",
    "    full_train_data = pd.merge(full_data, train_class, on='ID', how='inner')\n",
    "    full_test_data = pd.merge(full_data, test_class, on='ID', how='inner')\n",
    "\n",
    "    predictors = select_predictors(\n",
    "        full_train_data,\n",
    "        target_column,\n",
    "        'T',\n",
    "        base2_feat_oh.shape[1] - 2,\n",
    "        threshold=feature_threshold,\n",
    "        col_all=False)\n",
    "    forest = create_rnd_forest()\n",
    "    svm = create_svc()\n",
    "    bag = create_bag_cls()\n",
    "    vt_classifier = VotingClassifier([\n",
    "        ('Random Forest', forest),\n",
    "        ('SVM', svm),\n",
    "        ('Bagging', bag)],\n",
    "        voting='hard')\n",
    "    vt_classifier = vt_classifier.fit(\n",
    "        full_train_data[predictors],\n",
    "        full_train_data[target_column])\n",
    "    test_pred = vt_classifier.predict(full_test_data[predictors])\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"ID\": full_test_data[\"ID\"],\n",
    "        \"TARGET\": test_pred.astype(int)\n",
    "    })\n",
    "    submission.to_csv(\"ChankovYehor_test.txt\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egorc\\AppData\\Local\\conda\\conda\\envs\\ipythonenv\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [  17   51   81   84   96  101  103  106  108  115  128  138  153  161  169\n",
      "  177  179  181  195  199  212  230  232  236  237  245  251  255  261  274\n",
      "  288  303  320  321  328  336  337  343  347  351  352  363  401  402  411\n",
      "  419  429  444  455  461  462  465  485  494  500  502  508  509  511  516\n",
      "  524  531  535  536  537  544  553  554  560  562  563  565  566  572  576\n",
      "  581  589  590  591  593  595  609  614  617  620  623  632  638  645  651\n",
      "  652  681  686  689  695  730  743  746  751  757  771  774  781  790  819\n",
      "  820  824  827  834  839  840  841  843  849  850  853  856  858  861  864\n",
      "  873  874  875  876  885  888  904  911  919  924  927  928  930  938  939\n",
      "  955  959  967  969  977  983 1002 1025 1028 1032 1033 1041 1051 1056 1070\n",
      " 1079 1086 1089 1090 1104 1113 1124 1144 1162 1163 1168 1172 1195 1197 1211\n",
      " 1212 1214 1216 1217 1221 1239 1253 1265 1274 1279 1289 1296 1297 1304 1315\n",
      " 1332 1341 1347 1353 1358 1359 1367 1369 1378 1381 1385 1401 1409 1410 1411\n",
      " 1414 1421 1427 1429 1430 1443 1445 1448 1460 1473 1475 1479 1481 1489 1491\n",
      " 1494 1495 1498 1499 1506 1522 1533 1536 1538 1540 1552 1553 1554 1555 1578\n",
      " 1580 1594 1602 1610] are constant.\n",
      "  UserWarning)\n",
      "C:\\Users\\egorc\\AppData\\Local\\conda\\conda\\envs\\ipythonenv\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\Users\\egorc\\AppData\\Local\\conda\\conda\\envs\\ipythonenv\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in greater_equal\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
